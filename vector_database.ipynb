{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-3.17.3-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading pypdf-3.17.3-py3-none-any.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/277.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/277.9 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/277.9 kB 393.8 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 112.6/277.9 kB 819.2 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 194.6/277.9 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 277.9/277.9 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-3.17.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  1. Python.pdf\n",
      "Processing file:  10. Getting your first Data Science Job.pdf\n",
      "Processing file:  2. R.pdf\n",
      "Processing file:  3. Docker Container-Part 1.pdf\n",
      "Processing file:  3. SQL.pdf\n",
      "Processing file:  4. Data Science.pdf\n",
      "Processing file:  5. Machine Learning.pdf\n",
      "Processing file:  6. Kubernates-Part-2 (1).pdf\n",
      "Processing file:  6. Kubernates-Part1.pdf\n",
      "Processing file:  6. Supervised Learning.pdf\n",
      "Processing file:  7. Statistics.pdf\n",
      "Processing file:  7. Terraform.pdf\n",
      "Processing file:  8. Data Engineering.pdf\n",
      "Processing file:  9. GIT.pdf\n",
      "Processing file:  Algorithms for decision making.pdf\n",
      "Processing file:  Andrew NG_Lecture_Notes.pdf\n",
      "Processing file:  Application_through_Production_slides.pdf\n",
      "Processing file:  Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\n",
      "Processing file:  building-machine-learning-powered-applications-going-from-idea-to-product.pdf\n",
      "Processing file:  Data science 273 page.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from utils import load_pdfs, get_chunks\n",
    "\n",
    "\n",
    "def remove_surrogates(text):\n",
    "    return text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "\n",
    "\n",
    "pdf_chunks = []\n",
    "\n",
    "\n",
    "for path in os.listdir(\"data\"):\n",
    "    print(\"Processing file: \", path)\n",
    "    if path.endswith(\".pdf\"):\n",
    "        pdf_content = load_pdfs(os.path.join(\"data\", path))\n",
    "        splitted_text = get_chunks(pdf_content)\n",
    "        clean_text = [remove_surrogates(text) for text in splitted_text]\n",
    "        pdf_chunks.extend(clean_text)\n",
    "\n",
    "# need to implement markdown reader. Problem with NLTK\n",
    "\n",
    "persist_directory = os.getenv(\"PERSIST_DIRECTORY\", \"VectorDB\")\n",
    "\n",
    "# Initialise embeddings - we can alternatively chose one of many different open-source embeddings\n",
    "# Initialize model\n",
    "# model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create vector DB using Chroma\n",
    "print(\"Creating vector DB\")\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=pdf_chunks, embedding=embedding, persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Example use case\n",
    "question = \"what is reguralisation?\"\n",
    "\n",
    "#Loading from disk\n",
    "db3 = Chroma(persist_directory=\"docs/chroma/\", embedding_function=embedding)\n",
    "docs = db3.similarity_search(question, k=3)\n",
    "\n",
    "print(docs)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(path):\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    pdf_content = \"\\n\".join([page.page_content for page in pages])\n",
    "    return pdf_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(text, chunk_size=1000, chunk_overlap=10):\n",
    "   \n",
    "    r_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n \\n\",\"\\n\"]\n",
    "    )\n",
    "    split_data = r_splitter.split_text(text)\n",
    "    return split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_chunks = []\n",
    "for path in os.listdir(\"../data/\"):\n",
    "    if path.endswith(\".pdf\"):\n",
    "        pdf_content = load_pdfs(os.path.join(\"../data\", path))\n",
    "        splitted_text = get_chunks(pdf_content)\n",
    "        pdf_chunks.extend(splitted_text)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49091"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Lohit Reddy/nltk_data'\n    - 'd:\\\\Work\\\\Projects\\\\Softwares\\\\envs\\\\NLP\\\\nltk_data'\n    - 'd:\\\\Work\\\\Projects\\\\Softwares\\\\envs\\\\NLP\\\\share\\\\nltk_data'\n    - 'd:\\\\Work\\\\Projects\\\\Softwares\\\\envs\\\\NLP\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lohit Reddy\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Work\\Projects\\5.DS_interview_langchain\\vector_database.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m, path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m path\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.md\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     text \u001b[39m=\u001b[39m load_markdowns(path)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     mark_down_docs\u001b[39m.\u001b[39mappend(text)\n",
      "\u001b[1;32md:\\Work\\Projects\\5.DS_interview_langchain\\vector_database.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_markdowns\u001b[39m(path):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     loader \u001b[39m=\u001b[39m UnstructuredMarkdownLoader(path, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     text \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\langchain\\document_loaders\\unstructured.py:86\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m     85\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     elements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_elements()\n\u001b[0;32m     87\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_process_elements(elements)\n\u001b[0;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\langchain\\document_loaders\\markdown.py:45\u001b[0m, in \u001b[0;36mUnstructuredMarkdownLoader._get_elements\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m unstructured_version \u001b[39m<\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m16\u001b[39m):\n\u001b[0;32m     40\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     41\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are on unstructured version \u001b[39m\u001b[39m{\u001b[39;00m__unstructured_version__\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPartitioning markdown files is only supported in unstructured>=0.4.16.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     43\u001b[0m     )\n\u001b[1;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m partition_md(filename\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munstructured_kwargs)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\documents\\elements.py:503\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    502\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: _P\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: _P\u001b[39m.\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Element]:\n\u001b[1;32m--> 503\u001b[0m     elements \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    504\u001b[0m     sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m    505\u001b[0m     params: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:591\u001b[0m, in \u001b[0;36madd_filetype.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    590\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: _P\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: _P\u001b[39m.\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Element]:\n\u001b[1;32m--> 591\u001b[0m     elements \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    592\u001b[0m     sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m    593\u001b[0m     params: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:546\u001b[0m, in \u001b[0;36madd_metadata.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    545\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: _P\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: _P\u001b[39m.\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Element]:\n\u001b[1;32m--> 546\u001b[0m     elements \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    547\u001b[0m     sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m    548\u001b[0m     params: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\chunking\\title.py:241\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: _P\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: _P\u001b[39m.\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Element]:\n\u001b[1;32m--> 241\u001b[0m     elements \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    242\u001b[0m     sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m    243\u001b[0m     params: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\partition\\md.py:104\u001b[0m, in \u001b[0;36mpartition_md\u001b[1;34m(filename, file, text, url, include_page_breaks, include_metadata, parser, metadata_filename, metadata_last_modified, chunking_strategy, languages, detect_language_per_element, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     text \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mtext\n\u001b[0;32m    102\u001b[0m html \u001b[39m=\u001b[39m markdown\u001b[39m.\u001b[39mmarkdown(text, extensions\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mtables\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m--> 104\u001b[0m \u001b[39mreturn\u001b[39;00m partition_html(\n\u001b[0;32m    105\u001b[0m     text\u001b[39m=\u001b[39;49mhtml,\n\u001b[0;32m    106\u001b[0m     include_page_breaks\u001b[39m=\u001b[39;49minclude_page_breaks,\n\u001b[0;32m    107\u001b[0m     include_metadata\u001b[39m=\u001b[39;49minclude_metadata,\n\u001b[0;32m    108\u001b[0m     parser\u001b[39m=\u001b[39;49mparser,\n\u001b[0;32m    109\u001b[0m     source_format\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmd\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    110\u001b[0m     metadata_filename\u001b[39m=\u001b[39;49mmetadata_filename,\n\u001b[0;32m    111\u001b[0m     metadata_last_modified\u001b[39m=\u001b[39;49mmetadata_last_modified \u001b[39mor\u001b[39;49;00m last_modification_date,\n\u001b[0;32m    112\u001b[0m     languages\u001b[39m=\u001b[39;49mlanguages,\n\u001b[0;32m    113\u001b[0m     detect_language_per_element\u001b[39m=\u001b[39;49mdetect_language_per_element,\n\u001b[0;32m    114\u001b[0m     detection_origin\u001b[39m=\u001b[39;49mDETECTION_ORIGIN,\n\u001b[0;32m    115\u001b[0m )\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\documents\\elements.py:503\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    502\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: _P\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: _P\u001b[39m.\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Element]:\n\u001b[1;32m--> 503\u001b[0m     elements \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    504\u001b[0m     sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m    505\u001b[0m     params: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:591\u001b[0m, in \u001b[0;36madd_filetype.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    590\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: _P\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: _P\u001b[39m.\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Element]:\n\u001b[1;32m--> 591\u001b[0m     elements \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    592\u001b[0m     sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m    593\u001b[0m     params: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:546\u001b[0m, in \u001b[0;36madd_metadata.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    545\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: _P\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: _P\u001b[39m.\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Element]:\n\u001b[1;32m--> 546\u001b[0m     elements \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    547\u001b[0m     sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m    548\u001b[0m     params: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\chunking\\title.py:241\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs: _P\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: _P\u001b[39m.\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Element]:\n\u001b[1;32m--> 241\u001b[0m     elements \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    242\u001b[0m     sig \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(func)\n\u001b[0;32m    243\u001b[0m     params: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args)), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\partition\\html.py:141\u001b[0m, in \u001b[0;36mpartition_html\u001b[1;34m(filename, file, text, url, encoding, include_page_breaks, include_metadata, headers, ssl_verify, parser, source_format, html_assemble_articles, metadata_filename, metadata_last_modified, skip_headers_and_footers, chunking_strategy, languages, detect_language_per_element, detection_origin, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m skip_headers_and_footers:\n\u001b[0;32m    137\u001b[0m     document \u001b[39m=\u001b[39m filter_footer_and_header(document)\n\u001b[0;32m    139\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\n\u001b[0;32m    140\u001b[0m     apply_lang_metadata(\n\u001b[1;32m--> 141\u001b[0m         document_to_element_list(\n\u001b[0;32m    142\u001b[0m             document,\n\u001b[0;32m    143\u001b[0m             sortable\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    144\u001b[0m             include_page_breaks\u001b[39m=\u001b[39;49minclude_page_breaks,\n\u001b[0;32m    145\u001b[0m             last_modification_date\u001b[39m=\u001b[39;49mmetadata_last_modified \u001b[39mor\u001b[39;49;00m last_modification_date,\n\u001b[0;32m    146\u001b[0m             source_format\u001b[39m=\u001b[39;49msource_format \u001b[39mif\u001b[39;49;00m source_format \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    147\u001b[0m             detection_origin\u001b[39m=\u001b[39;49mdetection_origin,\n\u001b[0;32m    148\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    149\u001b[0m         ),\n\u001b[0;32m    150\u001b[0m         languages\u001b[39m=\u001b[39mlanguages,\n\u001b[0;32m    151\u001b[0m         detect_language_per_element\u001b[39m=\u001b[39mdetect_language_per_element,\n\u001b[0;32m    152\u001b[0m     ),\n\u001b[0;32m    153\u001b[0m )\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\partition\\common.py:559\u001b[0m, in \u001b[0;36mdocument_to_element_list\u001b[1;34m(document, sortable, include_page_breaks, last_modification_date, infer_list_items, source_format, detection_origin, sort_mode, languages, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Converts a DocumentLayout object to a list of unstructured elements.\"\"\"\u001b[39;00m\n\u001b[0;32m    557\u001b[0m elements: List[Element] \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 559\u001b[0m num_pages \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(document\u001b[39m.\u001b[39;49mpages)\n\u001b[0;32m    560\u001b[0m \u001b[39mfor\u001b[39;00m i, page \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(document\u001b[39m.\u001b[39mpages):\n\u001b[0;32m    561\u001b[0m     page_elements: List[Element] \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\documents\\xml.py:54\u001b[0m, in \u001b[0;36mXMLDocument.pages\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Gets all elements from pages in sequential order.\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pages \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_pages_from_element_tree()\n\u001b[0;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mpages\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\documents\\html.py:176\u001b[0m, in \u001b[0;36mHTMLDocument._parse_pages_from_element_tree\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m             page\u001b[39m.\u001b[39melements\u001b[39m.\u001b[39mappend(element)\n\u001b[0;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     element \u001b[39m=\u001b[39m _parse_tag(tag_elem)\n\u001b[0;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m element \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m         page\u001b[39m.\u001b[39melements\u001b[39m.\u001b[39mappend(element)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\documents\\html.py:410\u001b[0m, in \u001b[0;36m_parse_tag\u001b[1;34m(tag_elem)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m text:\n\u001b[0;32m    409\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m \u001b[39mreturn\u001b[39;00m _text_to_element(\n\u001b[0;32m    411\u001b[0m     text,\n\u001b[0;32m    412\u001b[0m     tag_elem\u001b[39m.\u001b[39;49mtag,\n\u001b[0;32m    413\u001b[0m     ancestortags,\n\u001b[0;32m    414\u001b[0m     links\u001b[39m=\u001b[39;49mlinks,\n\u001b[0;32m    415\u001b[0m     emphasized_texts\u001b[39m=\u001b[39;49memphasized_texts,\n\u001b[0;32m    416\u001b[0m     depth\u001b[39m=\u001b[39;49mdepth,\n\u001b[0;32m    417\u001b[0m )\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\documents\\html.py:458\u001b[0m, in \u001b[0;36m_text_to_element\u001b[1;34m(text, tag, ancestortags, depth, links, emphasized_texts)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(text) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 458\u001b[0m \u001b[39melif\u001b[39;00m is_narrative_tag(text, tag):\n\u001b[0;32m    459\u001b[0m     \u001b[39mreturn\u001b[39;00m HTMLNarrativeText(\n\u001b[0;32m    460\u001b[0m         text,\n\u001b[0;32m    461\u001b[0m         tag\u001b[39m=\u001b[39mtag,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    464\u001b[0m         emphasized_texts\u001b[39m=\u001b[39memphasized_texts,\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m \u001b[39melif\u001b[39;00m is_heading_tag(tag) \u001b[39mor\u001b[39;00m is_possible_title(text):\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\documents\\html.py:506\u001b[0m, in \u001b[0;36mis_narrative_tag\u001b[1;34m(text, tag)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_narrative_tag\u001b[39m(text: \u001b[39mstr\u001b[39m, tag: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m    505\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uses tag information to infer whether text is narrative.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m tag \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m HEADING_TAGS \u001b[39mand\u001b[39;00m is_possible_narrative_text(text)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\partition\\text_type.py:77\u001b[0m, in \u001b[0;36mis_possible_narrative_text\u001b[1;34m(text, cap_threshold, non_alpha_threshold, languages, language_checks)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m# NOTE(robinson): it gets read in from the environment as a string so we need to\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[39m# cast it to a float\u001b[39;00m\n\u001b[0;32m     74\u001b[0m cap_threshold \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\n\u001b[0;32m     75\u001b[0m     os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mUNSTRUCTURED_NARRATIVE_TEXT_CAP_THRESHOLD\u001b[39m\u001b[39m\"\u001b[39m, cap_threshold),\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[39mif\u001b[39;00m exceeds_cap_ratio(text, threshold\u001b[39m=\u001b[39;49mcap_threshold):\n\u001b[0;32m     78\u001b[0m     trace_logger\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNot narrative. Text exceeds cap ratio \u001b[39m\u001b[39m{\u001b[39;00mcap_threshold\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)  \u001b[39m# type: ignore # noqa: E501\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\partition\\text_type.py:273\u001b[0m, in \u001b[0;36mexceeds_cap_ratio\u001b[1;34m(text, threshold)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Checks the title ratio in a section of text. If a sufficient proportion of the words\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[39mare capitalized, that can be indicated on non-narrative text (i.e. \"1A. Risk Factors\").\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39m    the function returns True\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m# NOTE(robinson) - Currently limiting this to only sections of text with one sentence.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m# The assumption is that sections with multiple sentences are not titles.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m \u001b[39mif\u001b[39;00m sentence_count(text, \u001b[39m3\u001b[39;49m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39mif\u001b[39;00m text\u001b[39m.\u001b[39misupper():\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\partition\\text_type.py:222\u001b[0m, in \u001b[0;36msentence_count\u001b[1;34m(text, min_length)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentence_count\u001b[39m(text: \u001b[39mstr\u001b[39m, min_length: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m    212\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Checks the sentence count for a section of text. Titles should not be more than one\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m    sentence.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39m        The min number of words a section needs to be for it to be considered a sentence.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     sentences \u001b[39m=\u001b[39m sent_tokenize(text)\n\u001b[0;32m    223\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    224\u001b[0m     \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences:\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\unstructured\\nlp\\tokenize.py:30\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A wrapper around the NLTK sentence tokenizer with LRU caching enabled.\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m _download_nltk_package_if_not_present(package_category\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers\u001b[39m\u001b[39m\"\u001b[39m, package_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpunkt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m _sent_tokenize(text)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Lohit Reddy/nltk_data'\n    - 'd:\\\\Work\\\\Projects\\\\Softwares\\\\envs\\\\NLP\\\\nltk_data'\n    - 'd:\\\\Work\\\\Projects\\\\Softwares\\\\envs\\\\NLP\\\\share\\\\nltk_data'\n    - 'd:\\\\Work\\\\Projects\\\\Softwares\\\\envs\\\\NLP\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lohit Reddy\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# def load_markdowns(path):\n",
    "#     loader = UnstructuredMarkdownLoader(path, mode=\"elements\")\n",
    "#     text = loader.load()\n",
    "#     return text\n",
    "\n",
    "# mark_down_docs=[]\n",
    "# for path in os.listdir(\"data/\"):\n",
    "#     path = os.path.join(\"data\", path)\n",
    "#     if path.endswith(\".md\"):\n",
    "#         text = load_markdowns(path)\n",
    "#         mark_down_docs.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "# splitter = MarkdownHeaderTextSplitter(\n",
    "# headers_to_split_on=[\n",
    "# (\"#\", \"Header 1\"), \n",
    "# (\"##\", \"Header 2\")\n",
    "# ]  \n",
    "# )\n",
    "\n",
    "# mark_down_chunks= []\n",
    "# for markdown_text in mark_down_docs:\n",
    "#     splits = splitter.split_text(markdown_text)\n",
    "#     mark_down_chunks.extend(splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPENAI Vector Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # Initialize model\n",
    "# model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = '../docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception occurred invoking consumer for subscription d3f193baaab64581ad2358e2b4c8159ato topic persistent://default/default/db595df3-2eca-4cdc-961c-3714f5b8214f 'utf-8' codec can't encode character '\\ud835' in position 34: surrogates not allowed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectordb = Chroma.from_texts(\n",
    "    texts=pdf_chunks,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71520\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do you deal with missing data?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q22. During analysis, how do you treat missing values?'),\n",
       " Document(page_content='Q22. During analysis, how do you treat missing values?'),\n",
       " Document(page_content='missing values. How will you deal with them? \\n \\nIf the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; \\nwe use the rest of the data to predict the values.'),\n",
       " Document(page_content='109. You are given a data set consisting of variables with more than 30 percent missing \\nvalues. How will you deal with them?  \\nThe following are ways to  handle missing data values:  \\nIf the data set is large, we can just simply remove the rows with missing data values. It is the quickest way;'),\n",
       " Document(page_content='5. Validate the model using a ne w data set.  \\n6. Start implementing the model and track the result to analyze the performance of the model over the \\nperiod of time.  \\nQ59. During analysis, how do you treat missing values?'),\n",
       " Document(page_content='Q6. You are given a data set consisting of variables with more than 30 percent \\nmissing values. How will you deal with them? \\n \\nIf the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; \\nwe use the rest of the data to predict the values.'),\n",
       " Document(page_content='The output of the above code is as shown:  \\n \\n109. You are given a data set consisting of variables with more than 30 percent missing \\nvalues. How will you deal with them?  \\nThe following are ways to  handle missing data values:'),\n",
       " Document(page_content='Q6 How Do You Handle Missing or Corrupted Data in a Dataset? \\nOne of the easiest ways to handle missing or corrupted data is to drop those rows or columns or                   \\nreplace them entirely with some other value. \\nThere are two useful methods in Pandas:'),\n",
       " Document(page_content='If the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; \\nwe use the rest of the data to predict the values.  \\nFor smaller data sets, we can substitute missing values wi th the mean or average of the rest o f the data'),\n",
       " Document(page_content='Q6. YOU ARE GIVEN A DATA SET CONSISTING OF VARIABLES WITH MORE THAN 30 PERCENT MISSING VALUES . HOW WILL YOU \\nDEAL WITH THEM ? ....................................................................................................................................................... 87')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q22. During analysis, how do you treat missing values?'),\n",
       " Document(page_content='Q22. During analysis, how do you treat missing values?'),\n",
       " Document(page_content='Q22. During analysis, how do you treat missing values?'),\n",
       " Document(page_content='missing values. How will you deal with them? \\n \\nIf the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; \\nwe use the rest of the data to predict the values.'),\n",
       " Document(page_content='109. You are given a data set consisting of variables with more than 30 percent missing \\nvalues. How will you deal with them?  \\nThe following are ways to  handle missing data values:  \\nIf the data set is large, we can just simply remove the rows with missing data values. It is the quickest way;')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading from disk\n",
    "db3 = Chroma(persist_directory=\"../docs/chroma\", embedding_function=embedding)\n",
    "docs = db3.similarity_search(question, k=5)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db3.similarity_search(question, k=20)\n",
    "relevant_passage =[]\n",
    "for doc in docs:\n",
    "    relevant_passage.append(doc.page_content)\n",
    "r = list(set(relevant_passage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_passage(db,question):\n",
    "    relevant_passage = db.similarity_search(question,k=20)\n",
    "    relevant=[]\n",
    "    for doc in relevant_passage:\n",
    "        relevant.append(doc.page_content)\n",
    "    relevant_passage = list(set(relevant))\n",
    "    doc=\"\"\n",
    "    for passage in relevant_passage:\n",
    "        doc = doc+passage\n",
    "    relevant_passage = doc\n",
    "    #relevant_passage = create_answer(question, relevant_passage)\n",
    "    #other methods get_relevant_documents\n",
    "    \n",
    "    return relevant_passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'missing values. How will you deal with them? \\n \\nIf the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; \\nwe use the rest of the data to predict the values.If the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; \\nwe use the rest of the data to predict the values.  \\nFor smaller data sets, we can substitute missing values wi th the mean or average of the rest o f the data109. You are given a data set consisting of variables with more than 30 percent missing \\nvalues. How will you deal with them?  \\nThe following are ways to  handle missing data values:  \\nIf the data set is large, we can just simply remove the rows with missing data values. It is the quickest way;Q22. During analysis, how do you treat missing values?Q6 How Do You Handle Missing or Corrupted Data in a Dataset? \\nOne of the easiest ways to handle missing or corrupted data is to drop those rows or columns or                   \\nreplace them entirely with some other value. \\nThere are two useful methods in Pandas:5. Validate the model using a ne w data set.  \\n6. Start implementing the model and track the result to analyze the performance of the model over the \\nperiod of time.  \\nQ59. During analysis, how do you treat missing values?The output of the above code is as shown:  \\n \\n109. You are given a data set consisting of variables with more than 30 percent missing \\nvalues. How will you deal with them?  \\nThe following are ways to  handle missing data values:Q6. YOU ARE GIVEN A DATA SET CONSISTING OF VARIABLES WITH MORE THAN 30 PERCENT MISSING VALUES . HOW WILL YOU \\nDEAL WITH THEM ? ....................................................................................................................................................... 875. Validate the model using a new data set. \\n6. Start implementing the model and track the result to analyze the performance of the model over \\nthe period of time. \\n \\nQ22. During analysis, how do you treat missing values?Q6. You are given a data set consisting of variables with more than 30 percent \\nmissing values. How will you deal with them? \\n \\nIf the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; \\nwe use the rest of the data to predict the values.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_relevant_passage(db3,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Work\\Projects\\5.DS_interview_langchain\\Experiment_notebooks\\vector_database.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/Experiment_notebooks/vector_database.ipynb#X43sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m docs\u001b[39m.\u001b[39;49mto_list()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_list'"
     ]
    }
   ],
   "source": [
    "docs.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for Document\npage_content\n  none is not an allowed value (type=type_error.none.not_allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\Work\\Projects\\5.DS_interview_langchain\\Experiment_notebooks\\vector_database.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/Experiment_notebooks/vector_database.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m db3\u001b[39m.\u001b[39;49mmax_marginal_relevance_search(question,k\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\langchain\\vectorstores\\chroma.py:554\u001b[0m, in \u001b[0;36mChroma.max_marginal_relevance_search\u001b[1;34m(self, query, k, fetch_k, lambda_mult, filter, where_document, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    550\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFor MMR search, you must specify an embedding function on\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcreation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m     )\n\u001b[0;32m    553\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function\u001b[39m.\u001b[39membed_query(query)\n\u001b[1;32m--> 554\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_marginal_relevance_search_by_vector(\n\u001b[0;32m    555\u001b[0m     embedding,\n\u001b[0;32m    556\u001b[0m     k,\n\u001b[0;32m    557\u001b[0m     fetch_k,\n\u001b[0;32m    558\u001b[0m     lambda_mult\u001b[39m=\u001b[39;49mlambda_mult,\n\u001b[0;32m    559\u001b[0m     \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m,\n\u001b[0;32m    560\u001b[0m     where_document\u001b[39m=\u001b[39;49mwhere_document,\n\u001b[0;32m    561\u001b[0m )\n\u001b[0;32m    562\u001b[0m \u001b[39mreturn\u001b[39;00m docs\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\langchain\\vectorstores\\chroma.py:516\u001b[0m, in \u001b[0;36mChroma.max_marginal_relevance_search_by_vector\u001b[1;34m(self, embedding, k, fetch_k, lambda_mult, filter, where_document, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__query_collection(\n\u001b[0;32m    503\u001b[0m     query_embeddings\u001b[39m=\u001b[39membedding,\n\u001b[0;32m    504\u001b[0m     n_results\u001b[39m=\u001b[39mfetch_k,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m     include\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmetadatas\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdocuments\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdistances\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    509\u001b[0m mmr_selected \u001b[39m=\u001b[39m maximal_marginal_relevance(\n\u001b[0;32m    510\u001b[0m     np\u001b[39m.\u001b[39marray(embedding, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32),\n\u001b[0;32m    511\u001b[0m     results[\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m    512\u001b[0m     k\u001b[39m=\u001b[39mk,\n\u001b[0;32m    513\u001b[0m     lambda_mult\u001b[39m=\u001b[39mlambda_mult,\n\u001b[0;32m    514\u001b[0m )\n\u001b[1;32m--> 516\u001b[0m candidates \u001b[39m=\u001b[39m _results_to_docs(results)\n\u001b[0;32m    518\u001b[0m selected_results \u001b[39m=\u001b[39m [r \u001b[39mfor\u001b[39;00m i, r \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(candidates) \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m mmr_selected]\n\u001b[0;32m    519\u001b[0m \u001b[39mreturn\u001b[39;00m selected_results\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\langchain\\vectorstores\\chroma.py:36\u001b[0m, in \u001b[0;36m_results_to_docs\u001b[1;34m(results)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_results_to_docs\u001b[39m(results: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m [doc \u001b[39mfor\u001b[39;00m doc, _ \u001b[39min\u001b[39;00m _results_to_docs_and_scores(results)]\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\langchain\\vectorstores\\chroma.py:40\u001b[0m, in \u001b[0;36m_results_to_docs_and_scores\u001b[1;34m(results)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_results_to_docs_and_scores\u001b[39m(results: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[Document, \u001b[39mfloat\u001b[39m]]:\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     41\u001b[0m         \u001b[39m# TODO: Chroma can do batch querying,\u001b[39;49;00m\n\u001b[0;32m     42\u001b[0m         \u001b[39m# we shouldn't hard code to the 1st result\u001b[39;49;00m\n\u001b[0;32m     43\u001b[0m         (Document(page_content\u001b[39m=\u001b[39;49mresult[\u001b[39m0\u001b[39;49m], metadata\u001b[39m=\u001b[39;49mresult[\u001b[39m1\u001b[39;49m] \u001b[39mor\u001b[39;49;00m {}), result[\u001b[39m2\u001b[39;49m])\n\u001b[0;32m     44\u001b[0m         \u001b[39mfor\u001b[39;49;00m result \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(\n\u001b[0;32m     45\u001b[0m             results[\u001b[39m\"\u001b[39;49m\u001b[39mdocuments\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m],\n\u001b[0;32m     46\u001b[0m             results[\u001b[39m\"\u001b[39;49m\u001b[39mmetadatas\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m],\n\u001b[0;32m     47\u001b[0m             results[\u001b[39m\"\u001b[39;49m\u001b[39mdistances\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m],\n\u001b[0;32m     48\u001b[0m         )\n\u001b[0;32m     49\u001b[0m     ]\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\langchain\\vectorstores\\chroma.py:43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_results_to_docs_and_scores\u001b[39m(results: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[Document, \u001b[39mfloat\u001b[39m]]:\n\u001b[0;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m     41\u001b[0m         \u001b[39m# TODO: Chroma can do batch querying,\u001b[39;00m\n\u001b[0;32m     42\u001b[0m         \u001b[39m# we shouldn't hard code to the 1st result\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m         (Document(page_content\u001b[39m=\u001b[39;49mresult[\u001b[39m0\u001b[39;49m], metadata\u001b[39m=\u001b[39;49mresult[\u001b[39m1\u001b[39;49m] \u001b[39mor\u001b[39;49;00m {}), result[\u001b[39m2\u001b[39m])\n\u001b[0;32m     44\u001b[0m         \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\n\u001b[0;32m     45\u001b[0m             results[\u001b[39m\"\u001b[39m\u001b[39mdocuments\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m     46\u001b[0m             results[\u001b[39m\"\u001b[39m\u001b[39mmetadatas\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m     47\u001b[0m             results[\u001b[39m\"\u001b[39m\u001b[39mdistances\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m     48\u001b[0m         )\n\u001b[0;32m     49\u001b[0m     ]\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\langchain\\load\\serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for Document\npage_content\n  none is not an allowed value (type=type_error.none.not_allowed)"
     ]
    }
   ],
   "source": [
    "db3.max_marginal_relevance_search(question,k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using Palm embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "import google.generativeai as palm\n",
    "# llm = GooglePalm(google_api_key = os.getenv('PALM_API_KEY'), temperature = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm.configure(api_key=os.getenv('PALM_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name='models/embedding-gecko-001',\n",
       "      base_model_id='',\n",
       "      version='001',\n",
       "      display_name='Embedding Gecko',\n",
       "      description='Obtain a distributed representation of a text.',\n",
       "      input_token_limit=1024,\n",
       "      output_token_limit=1,\n",
       "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
       "      temperature=None,\n",
       "      top_p=None,\n",
       "      top_k=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [m for m in palm.list_models() if 'embedText' in m.supported_generation_methods]\n",
    "model = models[0]\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.api.types import Documents, Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_function(texts: Documents) -> Embeddings:\n",
    "  # Embed the documents using any supported method\n",
    "  return  [palm.generate_embeddings(model=model, text=text)['embedding']\n",
    "           for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chroma_db(documents, name):\n",
    "  chroma_client = chromadb.Client()\n",
    "  db = chroma_client.create_collection(name=name, embedding_function=embed_function,)\n",
    "  for i,d in enumerate(documents):\n",
    "    db.add(\n",
    "      documents=d,\n",
    "      ids=str(i)\n",
    "    )\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_chroma_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Work\\Projects\\5.DS_interview_langchain\\vector_database.ipynb Cell 26\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m directory \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mVectorDB/palm_db/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpalm_db\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vectordb \u001b[39m=\u001b[39m Chroma\u001b[39m.\u001b[39;49mfrom_texts(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m texts\u001b[39m=\u001b[39;49mpdf_chunks,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m embedding\u001b[39m=\u001b[39;49membed_function,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m persist_directory\u001b[39m=\u001b[39;49mdirectory,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m collection_name\u001b[39m=\u001b[39;49mname\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work/Projects/5.DS_interview_langchain/vector_database.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\langchain\\vectorstores\\chroma.py:729\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m create_batches\n\u001b[0;32m    723\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m create_batches(\n\u001b[0;32m    724\u001b[0m         api\u001b[39m=\u001b[39mchroma_collection\u001b[39m.\u001b[39m_client,\n\u001b[0;32m    725\u001b[0m         ids\u001b[39m=\u001b[39mids,\n\u001b[0;32m    726\u001b[0m         metadatas\u001b[39m=\u001b[39mmetadatas,\n\u001b[0;32m    727\u001b[0m         documents\u001b[39m=\u001b[39mtexts,\n\u001b[0;32m    728\u001b[0m     ):\n\u001b[1;32m--> 729\u001b[0m         chroma_collection\u001b[39m.\u001b[39;49madd_texts(\n\u001b[0;32m    730\u001b[0m             texts\u001b[39m=\u001b[39;49mbatch[\u001b[39m3\u001b[39;49m] \u001b[39mif\u001b[39;49;00m batch[\u001b[39m3\u001b[39;49m] \u001b[39melse\u001b[39;49;00m [],\n\u001b[0;32m    731\u001b[0m             metadatas\u001b[39m=\u001b[39;49mbatch[\u001b[39m2\u001b[39;49m] \u001b[39mif\u001b[39;49;00m batch[\u001b[39m2\u001b[39;49m] \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    732\u001b[0m             ids\u001b[39m=\u001b[39;49mbatch[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    733\u001b[0m         )\n\u001b[0;32m    734\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     chroma_collection\u001b[39m.\u001b[39madd_texts(texts\u001b[39m=\u001b[39mtexts, metadatas\u001b[39m=\u001b[39mmetadatas, ids\u001b[39m=\u001b[39mids)\n",
      "File \u001b[1;32md:\\Work\\Projects\\Softwares\\envs\\NLP\\Lib\\site-packages\\langchain\\vectorstores\\chroma.py:275\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m texts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(texts)\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 275\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function\u001b[39m.\u001b[39;49membed_documents(texts)\n\u001b[0;32m    276\u001b[0m \u001b[39mif\u001b[39;00m metadatas:\n\u001b[0;32m    277\u001b[0m     \u001b[39m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    278\u001b[0m     \u001b[39m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     length_diff \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(texts) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(metadatas)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "directory = \"VectorDB/palm_db/\"\n",
    "name = \"palm_db\"\n",
    "vectordb = Chroma.from_texts(\n",
    "texts=pdf_chunks,\n",
    "embedding=embed_function,\n",
    "persist_directory=directory,\n",
    "collection_name=name\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
